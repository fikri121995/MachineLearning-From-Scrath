{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency - Inverse Document Frequency Vectorizer\n",
        "\n",
        "Z.W.Miller - Copyright 2018"
      ],
      "metadata": {
        "id": "f31Vzj8C-Z2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GWw9iXQkbo4d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import scipy \n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import matplotlib\n",
        "import sys\n",
        "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd), ('Scipy', scipy), ('Sklearn', sklearn))\n",
        "\n",
        "print(\"Python Version:\", sys.version, '\\n')\n",
        "for lib in libraries:\n",
        "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_h1QNq4_SQi",
        "outputId": "b5ccfbbd-da3d-4dd4-be63-4b6ee1fda078"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.7.15 (default, Oct 12 2022, 19:14:55) \n",
            "[GCC 7.5.0] \n",
            "\n",
            "Matplotlib Version: 3.2.2\n",
            "Numpy Version: 1.21.6\n",
            "Pandas Version: 1.3.5\n",
            "Scipy Version: 1.7.3\n",
            "Sklearn Version: 1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "\n",
        "class tfidf_vectorizer:\n",
        "    \n",
        "    def __init__(self, max_features=None, ngrams = (1,1), tokenizer=None, remove_stopwords=False):\n",
        "        \"\"\"\n",
        "        Term frequency, inverse document frequency vectorizer \n",
        "        reads the text provided, tokenizes it with the provided \n",
        "        tokenizer (or the default), then generates ngrams keeping \n",
        "        track of all ngrams as the vocabulary. Then it takes provided \n",
        "        texts and converts them into vectors by counting the \n",
        "        appearance of each ngram and tracking that for every document. \n",
        "        The counts are then scaled by the max term frequency and the\n",
        "        inverse document frequency (see converter method). This new\n",
        "        result is better than counts at picking out how important\n",
        "        words are based on both usage and uniqueness. \n",
        "        ---\n",
        "        KWargs:\n",
        "        max_features: how many ngrams to allow in the vector, using the\n",
        "        most common features first. If None, defaults to using all\n",
        "        ngrams (int)\n",
        "        ngrams: how many tokens to combine to form features. First element\n",
        "        of tuple is starting point, second is ending point.\n",
        "        tokenizer: what function to use to create tokens (must return \n",
        "        list of tokens)\n",
        "        remove_stopwords: whether to include very common english words that\n",
        "        do not add much value due to their commonness.\n",
        "        \"\"\"\n",
        "        self.max_features = max_features\n",
        "        self.vocabulary = {}\n",
        "        self.ngrams = ngrams\n",
        "        if tokenizer == None:\n",
        "            self.tokenizer = self.tokenize\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.stopwords = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
        "                          'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
        "                          'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', \n",
        "                          'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', \n",
        "                          'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
        "                          'themselves', 'until', 'below', 'are', 'we', 'these', 'your', \n",
        "                          'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', \n",
        "                          'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
        "                          'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', \n",
        "                          'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', \n",
        "                          'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', \n",
        "                          'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', \n",
        "                          'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', \n",
        "                          'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', \n",
        "                          'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', \n",
        "                          'was', 'here', 'than'}\n",
        "        \n",
        "    def token_generator(self, X):\n",
        "        \"\"\"\n",
        "        Generator that returns joined tokens as a single\n",
        "        string to act as a feature. It generates the tokens\n",
        "        by iterating through the allowed ngrams and combining\n",
        "        the appropriate number of tokens into a string.\n",
        "        \"\"\"\n",
        "        for i in range(self.ngrams[0],self.ngrams[1]+1):\n",
        "            for ix, _ in enumerate(X):\n",
        "                if ix+i < len(X)+1:\n",
        "                    yield ' '.join(X[ix:ix+i])\n",
        "    \n",
        "    def tokenize(self, X):\n",
        "        \"\"\"\n",
        "        Simple tokenizer that removes punctuation,\n",
        "        lowercases the text, and breaks on spaces.\n",
        "        Also removes stopwords and numeric values\n",
        "        from being treated as words.\n",
        "        \"\"\"\n",
        "        for symbol in punctuation:\n",
        "            X = X.replace(symbol,'')\n",
        "        final_token_list = [] \n",
        "        for token in X.lower().split():\n",
        "            if self.remove_stopwords:\n",
        "                if not self.check_stopwords(token):\n",
        "                    try:\n",
        "                        int(token)\n",
        "                        float(token)\n",
        "                    except:\n",
        "                        final_token_list.append(token)  \n",
        "            else:\n",
        "                final_token_list.append(token)\n",
        "        return final_token_list\n",
        "        \n",
        "    def check_stopwords(self, token):\n",
        "        \"\"\"\n",
        "        Checks if the token is in our list of common\n",
        "        stopwords, and returns a boolean.\n",
        "         \"\"\"\n",
        "        return token in self.stopwords\n",
        "    \n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Go through all provided training documents and\n",
        "        create the list of vocabulary for known documents\n",
        "        by looking at all ngrams and tracking how often\n",
        "        those ngrams appear. If max_features is defined,\n",
        "        only keep the most common tokens. Afterward,\n",
        "        generate a token_to_id mapper and an id_to_token\n",
        "        mapper.\n",
        "        \"\"\"\n",
        "        for document in X:\n",
        "            tokens = self.tokenizer(document)\n",
        "            for token in self.token_generator(tokens):\n",
        "                if token in self.vocabulary.keys():\n",
        "                    self.vocabulary[token] += 1\n",
        "                else:\n",
        "                    self.vocabulary[token] = 1\n",
        "        \n",
        "        if self.max_features != None:\n",
        "            temp_vocab = {}\n",
        "            for key, value in Counter(self.vocabulary).most_common(self.max_features):\n",
        "                temp_vocab[key] = value\n",
        "            self.vocabulary = temp_vocab\n",
        "            del temp_vocab\n",
        "            \n",
        "        self.token_to_id = {ky: ix for ix, ky in enumerate(sorted(self.vocabulary.keys()))}\n",
        "        self.id_to_token = {ix: ky for ix, ky in enumerate(sorted(self.vocabulary.keys()))}\n",
        "        \n",
        "        \n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Go through all provided documents and use the known\n",
        "        vocabulary to track how often each ngram appears in\n",
        "        the document. At the end, stack all of the generated\n",
        "        document vectors together. Convert them to tf-idf\n",
        "        and skip the initial vector that's all 0's, which \n",
        "        is just there to act as a template.\n",
        "        \"\"\"\n",
        "        vectorized_docs = np.zeros(len(self.vocabulary.keys()))\n",
        "        for document in X:\n",
        "            tokens = self.tokenizer(document)\n",
        "            vectorized_doc = np.zeros(len(self.vocabulary.keys()))\n",
        "            for token in self.token_generator(tokens):\n",
        "                if token in self.vocabulary:\n",
        "                    word_id = self.token_to_id[token]\n",
        "                    vectorized_doc[word_id] += 1\n",
        "            vectorized_docs = np.vstack((vectorized_docs,vectorized_doc))\n",
        "        return self.convert_counts_to_tf_idf(vectorized_docs)[1:]\n",
        "    def convert_counts_to_tf_idf(self, docs):\n",
        "        \"\"\"\n",
        "        To convert from counts to TF-IDF, we first scale\n",
        "        each value by the maximum in it's own column. This \n",
        "        lowers dependence on document length. Then we calculate\n",
        "        log(number of documents/(1+documents containing this ngram)).\n",
        "        This is the inverse document frequency (the one is to make\n",
        "        combat division by 0). Each value is scaled as:\n",
        "        term_frequency*inverse_document_frequency.\n",
        "        \"\"\"\n",
        "        number_of_columns = docs.shape[1]\n",
        "        number_of_docs = docs.shape[0]\n",
        "        frequency_scalers = np.ones(number_of_columns)\n",
        "        idf_terms = np.ones(number_of_columns)\n",
        "        for col in range(number_of_columns):\n",
        "            column_vals = docs.T[col]\n",
        "            frequency_scalers[col] = np.max(column_vals)\n",
        "            number_of_docs_containing = np.sum((column_vals > 0).astype(int))\n",
        "            idf_terms[col] = np.log(number_of_docs/(1+number_of_docs_containing))\n",
        "        docs = docs/frequency_scalers\n",
        "        docs = docs*idf_terms\n",
        "        \n",
        "        return docs    \n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Fit on X and then transform X and return it as vectors.\n",
        "        \"\"\"\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "                               \n",
        "    "
      ],
      "metadata": {
        "id": "yEmdnhg6_aJN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = tfidf_vectorizer(ngrams=(1,3), max_features=None)\n",
        "data = ['bob went to the store','dana, did not go to the store', 'the dog ran quickly toward the stoplight']\n",
        "cv.fit(data)"
      ],
      "metadata": {
        "id": "fnTKD9C2APnm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlnCm1tCAWG9",
        "outputId": "0a557fb3-4b42-4e63-857e-638dedfa769f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bob': 1,\n",
              " 'went': 1,\n",
              " 'to': 2,\n",
              " 'the': 4,\n",
              " 'store': 2,\n",
              " 'bob went': 1,\n",
              " 'went to': 1,\n",
              " 'to the': 2,\n",
              " 'the store': 2,\n",
              " 'bob went to': 1,\n",
              " 'went to the': 1,\n",
              " 'to the store': 2,\n",
              " 'dana': 1,\n",
              " 'did': 1,\n",
              " 'not': 1,\n",
              " 'go': 1,\n",
              " 'dana did': 1,\n",
              " 'did not': 1,\n",
              " 'not go': 1,\n",
              " 'go to': 1,\n",
              " 'dana did not': 1,\n",
              " 'did not go': 1,\n",
              " 'not go to': 1,\n",
              " 'go to the': 1,\n",
              " 'dog': 1,\n",
              " 'ran': 1,\n",
              " 'quickly': 1,\n",
              " 'toward': 1,\n",
              " 'stoplight': 1,\n",
              " 'the dog': 1,\n",
              " 'dog ran': 1,\n",
              " 'ran quickly': 1,\n",
              " 'quickly toward': 1,\n",
              " 'toward the': 1,\n",
              " 'the stoplight': 1,\n",
              " 'the dog ran': 1,\n",
              " 'dog ran quickly': 1,\n",
              " 'ran quickly toward': 1,\n",
              " 'quickly toward the': 1,\n",
              " 'toward the stoplight': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49-obUeIAbJC",
        "outputId": "52c16ad6-ce75-40af-94b8-eb37baec3aee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.69314718, 0.69314718, 0.69314718, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.28768207, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.28768207, 0.28768207, 0.28768207, 0.28768207, 0.        ,\n",
              "        0.        , 0.        , 0.69314718, 0.69314718, 0.69314718],\n",
              "       [0.        , 0.        , 0.        , 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.        ,\n",
              "        0.        , 0.        , 0.69314718, 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.69314718, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.28768207, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.28768207, 0.28768207, 0.28768207, 0.28768207, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
              "        0.        , 0.        , 0.69314718, 0.69314718, 0.69314718,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's turn on stopwords"
      ],
      "metadata": {
        "id": "TTWH1FR9AkTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = tfidf_vectorizer(ngrams=(1,3), max_features=None, remove_stopwords=True)\n",
        "data = ['bob went to the store','dana, did not go to the store','the dog ran quickly toward the stoplight']\n",
        "cv.fit(data)"
      ],
      "metadata": {
        "id": "VNOBUY14AuTw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtLXVaNeAx68",
        "outputId": "fbb92291-208a-46d2-a9ed-170cf6869b3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.69314718, 0.69314718, 0.69314718, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.28768207, 0.        ,\n",
              "        0.        , 0.69314718, 0.69314718],\n",
              "       [0.        , 0.        , 0.        , 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.        , 0.        , 0.        , 0.69314718,\n",
              "        0.69314718, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.28768207, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.69314718, 0.69314718, 0.69314718, 0.        ,\n",
              "        0.        , 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.69314718, 0.69314718, 0.        , 0.69314718,\n",
              "        0.69314718, 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now a larger dataset"
      ],
      "metadata": {
        "id": "ygsjQ1HoA5qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
        "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
        "                                       categories=categories, \n",
        "                                       remove=('headers', \n",
        "                                               'footers', 'quotes'))\n",
        "data = ng_train.data\n",
        "targets = ng_train.target"
      ],
      "metadata": {
        "id": "KU4nahvgA1Jx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = tfidf_vectorizer(ngrams=(1,3), max_features=100, remove_stopwords=True)\n",
        "cv.fit(data)"
      ],
      "metadata": {
        "id": "KAucE7BKBbnn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(cv.transform(data), columns=cv.token_to_id.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "-ufvxJ6HBfNO",
        "outputId": "62e20e41-4af9-48fd-b357-770e7c362029"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       3d      also  another  anyone  argument  atheists  available  back  \\\n",
              "0     0.0  0.060062      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "1     0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "2     0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "3     0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "4     0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "...   ...       ...      ...     ...       ...       ...        ...   ...   \n",
              "1656  0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "1657  0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "1658  0.0  0.000000      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "1659  0.0  0.000000      0.0     0.0   0.11267  0.049139        0.0   0.0   \n",
              "1660  0.0  0.060062      0.0     0.0   0.00000  0.000000        0.0   0.0   \n",
              "\n",
              "       believe      best  ...     using  version  want  way      well  \\\n",
              "0     0.000000  0.000000  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "1     0.000000  0.000000  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "2     0.000000  0.000000  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "3     0.000000  0.000000  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "4     0.000000  0.338281  ...  0.000000      0.0   0.0  0.0  0.107740   \n",
              "...        ...       ...  ...       ...      ...   ...  ...       ...   \n",
              "1656  0.000000  0.000000  ...  0.184121      0.0   0.0  0.0  0.000000   \n",
              "1657  0.000000  0.000000  ...  0.184121      0.0   0.0  0.0  0.000000   \n",
              "1658  0.000000  0.000000  ...  0.000000      0.0   0.0  0.0  0.215479   \n",
              "1659  0.101623  0.000000  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "1660  0.000000  0.338281  ...  0.000000      0.0   0.0  0.0  0.000000   \n",
              "\n",
              "      without      work     would      year  years  \n",
              "0         0.0  0.000000  0.000000  0.156879    0.0  \n",
              "1         0.0  0.000000  0.000000  0.000000    0.0  \n",
              "2         0.0  0.000000  0.000000  0.000000    0.0  \n",
              "3         0.0  0.000000  0.000000  0.000000    0.0  \n",
              "4         0.0  0.000000  0.156603  0.000000    0.0  \n",
              "...       ...       ...       ...       ...    ...  \n",
              "1656      0.0  0.273365  0.000000  0.000000    0.0  \n",
              "1657      0.0  0.000000  0.000000  0.000000    0.0  \n",
              "1658      0.0  0.000000  0.156603  0.000000    0.0  \n",
              "1659      0.0  0.000000  0.156603  0.000000    0.0  \n",
              "1660      0.0  0.000000  0.078301  0.000000    0.0  \n",
              "\n",
              "[1661 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49d4f945-6615-4390-b123-35b382a52cc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>3d</th>\n",
              "      <th>also</th>\n",
              "      <th>another</th>\n",
              "      <th>anyone</th>\n",
              "      <th>argument</th>\n",
              "      <th>atheists</th>\n",
              "      <th>available</th>\n",
              "      <th>back</th>\n",
              "      <th>believe</th>\n",
              "      <th>best</th>\n",
              "      <th>...</th>\n",
              "      <th>using</th>\n",
              "      <th>version</th>\n",
              "      <th>want</th>\n",
              "      <th>way</th>\n",
              "      <th>well</th>\n",
              "      <th>without</th>\n",
              "      <th>work</th>\n",
              "      <th>would</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.060062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156879</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.338281</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.107740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156603</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.184121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.273365</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1657</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.184121</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1658</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.215479</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156603</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1659</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.11267</td>\n",
              "      <td>0.049139</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.101623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156603</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.060062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.338281</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078301</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1661 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49d4f945-6615-4390-b123-35b382a52cc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49d4f945-6615-4390-b123-35b382a52cc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49d4f945-6615-4390-b123-35b382a52cc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see this a pretty sparse set of vectors, so if we wanted to store this in a smaller format for transferring, we could convert it to a sparse matrix"
      ],
      "metadata": {
        "id": "GWoU__7LBkfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = cv.transform(data)"
      ],
      "metadata": {
        "id": "m0O3VEWSB-Ij"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "sparse_matrix = sparse.csr_matrix(vectors)"
      ],
      "metadata": {
        "id": "2SqNRe7RCEuH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sparse_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qC9QfL_CVO9",
        "outputId": "f50a1f72-b719-43a2-eb8f-9cc2d2c701e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t0.06006227187458042\n",
            "  (0, 30)\t0.06765414491261239\n",
            "  (0, 44)\t0.2501101112768755\n",
            "  (0, 50)\t0.19098441633434454\n",
            "  (0, 53)\t0.20920851641474703\n",
            "  (0, 98)\t0.15687879615922845\n",
            "  (1, 58)\t0.03735065491569401\n",
            "  (2, 32)\t0.10077767550087349\n",
            "  (2, 44)\t0.2501101112768755\n",
            "  (2, 80)\t0.7409400415083943\n",
            "  (3, 32)\t0.050388837750436746\n",
            "  (3, 55)\t0.1455548016565918\n",
            "  (3, 73)\t0.2510502196976964\n",
            "  (4, 9)\t0.33828084676288245\n",
            "  (4, 12)\t0.23529843583598417\n",
            "  (4, 28)\t0.13921538689209098\n",
            "  (4, 45)\t0.3016996232914419\n",
            "  (4, 59)\t0.07616333023847875\n",
            "  (4, 79)\t1.6003751191852191\n",
            "  (4, 84)\t0.13301613174287383\n",
            "  (4, 94)\t0.10773963463799348\n",
            "  (4, 97)\t0.15660264620151787\n",
            "  (5, 3)\t0.6770939708754349\n",
            "  (5, 8)\t0.20324637217713937\n",
            "  (5, 12)\t0.23529843583598417\n",
            "  :\t:\n",
            "  (1658, 70)\t0.09381389084551936\n",
            "  (1658, 71)\t0.2158281603387612\n",
            "  (1658, 94)\t0.21547926927598696\n",
            "  (1658, 97)\t0.15660264620151787\n",
            "  (1659, 4)\t0.11266962652717288\n",
            "  (1659, 5)\t0.04913884354693559\n",
            "  (1659, 8)\t0.10162318608856968\n",
            "  (1659, 46)\t0.12274569577407939\n",
            "  (1659, 51)\t0.05670706246504851\n",
            "  (1659, 55)\t0.1455548016565918\n",
            "  (1659, 58)\t0.03735065491569401\n",
            "  (1659, 59)\t0.07616333023847875\n",
            "  (1659, 65)\t0.1786385684680599\n",
            "  (1659, 70)\t0.09381389084551936\n",
            "  (1659, 78)\t0.2848607457217529\n",
            "  (1659, 82)\t0.4015152556324717\n",
            "  (1659, 97)\t0.15660264620151787\n",
            "  (1660, 1)\t0.06006227187458042\n",
            "  (1660, 9)\t0.33828084676288245\n",
            "  (1660, 12)\t0.23529843583598417\n",
            "  (1660, 36)\t0.1637491518636658\n",
            "  (1660, 58)\t0.03735065491569401\n",
            "  (1660, 80)\t0.7409400415083943\n",
            "  (1660, 83)\t0.12818338380824157\n",
            "  (1660, 97)\t0.07830132310075893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use the output in a model"
      ],
      "metadata": {
        "id": "YCYOSh4uCdwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('../../zwml/naive_bayes/')"
      ],
      "metadata": {
        "id": "Mhum04ZWCn4i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RQQyg98GOSO",
        "outputId": "fde5e7ad-9d7e-47ad-e6c2-4a3b26ff965a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=4eaaa4fc16573d4e3aeec5fd8a918d5efbe95acaca23587f87c62d5b623446c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U numpy scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33r7aY_-iO24",
        "outputId": "dbf6de97-994a-4cc4-a46a-fc3a18d4c8db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinominalNB \n"
      ],
      "metadata": {
        "id": "4kO2w0BDC-KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(vectors, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqlY3YG_DDDp",
        "outputId": "689e0018-ebc4-486d-ff2c-629c74407e36"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.06006227, 0.        , ..., 0.        , 0.1568788 ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        ...,\n",
              "        [0.        , 0.        , 0.        , ..., 0.15660265, 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.15660265, 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.06006227, 0.        , ..., 0.07830132, 0.        ,\n",
              "         0.        ]]), array([2, 2, 0, ..., 0, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(vectors, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCv6mjkyDJ9P",
        "outputId": "2deecd47-1d87-45b0-8044-b72f3f3263ad"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.06006227, 0.        , ..., 0.        , 0.1568788 ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "         0.        ],\n",
              "        ...,\n",
              "        [0.        , 0.        , 0.        , ..., 0.15660265, 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.15660265, 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.06006227, 0.        , ..., 0.07830132, 0.        ,\n",
              "         0.        ]]), array([2, 2, 0, ..., 0, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dc = DummyClassifier()\n",
        "dc.fit(vectors,targets)\n",
        "dc.score(vectors,targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umV9UVQXDR7o",
        "outputId": "48ddad99-9244-422a-d9ce-873e4252d53c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35942203491872365"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}